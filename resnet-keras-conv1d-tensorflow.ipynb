{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    from keras.preprocessing import sequence\n",
    "    from keras.datasets import imdb\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = 5000)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen = 500, padding = \"post\", truncating = \"post\")\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen = 500, padding = \"post\", truncating = \"post\")\n",
    "    \n",
    "    vocabulary = imdb.get_word_index()\n",
    "    vocabulary_inv = dict((v, k) for k, v in vocabulary.items())\n",
    "    vocabulary_inv[0] = '<PAD/>'\n",
    "    return X_train, y_train, X_test, y_test, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, vocabulary_inv = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 500), (25000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentence_matrix, vocabulary_inv, num_features = 300, min_word_count = 1, context = 10):\n",
    "    from gensim.models import word2vec\n",
    "    num_workers = 2\n",
    "    downsampling = 1e-3\n",
    "    \n",
    "    print(\"Training Word2Vec model...\")\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers = num_workers, size = num_features, \n",
    "                                        min_count = min_word_count, window = context, sample = downsampling)\n",
    "    embedding_model.init_sims(replace = True)\n",
    "    \n",
    "    num_words = sentence_matrix.max()\n",
    "    embedding_weights = []\n",
    "    print(\"num_words : {}\".format(num_words))\n",
    "    for i in range(num_words + 1):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model:\n",
    "            embedding_weights.append(embedding_model[word])\n",
    "        else:\n",
    "            print(\"word : {}\".format(word))\n",
    "            embedding_weights.append(np.random.uniform(-0.25, 0.25, embedding_model.vector_size))\n",
    "    print(\"embedding_weights len : {}\".format(len(embedding_weights)))\n",
    "    return np.vstack(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "num_words : 4999\n",
      "word : a\n",
      "embedding_weights len : 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = train_word2vec(np.vstack((X_train, X_test)), vocabulary_inv, num_features = 50, min_word_count = 1, context = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def calc_recall_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def calc_precision_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def calc_f1_score(y_true, y_pred):\n",
    "    precision = calc_precision_score(y_true, y_pred)\n",
    "    recall = calc_recall_score(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bn_relu(input):\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.layers import Activation\n",
    "    norm = BatchNormalization(axis = 2)(input)\n",
    "    return Activation(\"relu\")(norm)\n",
    "\n",
    "\n",
    "def _bn_relu_conv(**conv_params):\n",
    "    from keras.regularizers import l2\n",
    "    from keras.layers import Conv1D\n",
    "    filters = conv_params['filters']\n",
    "    kernel_size = conv_params['kernel_size']\n",
    "    strides = conv_params.setdefault(\"strides\", 1)\n",
    "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
    "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
    "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1e-4))\n",
    "    \n",
    "    def f(input):\n",
    "        activation = _bn_relu(input)\n",
    "        return Conv1D(filters = filters, kernel_size = kernel_size,\n",
    "                     strides = strides, padding = padding,\n",
    "                     kernel_initializer = kernel_initializer,\n",
    "                     kernel_regularizer = kernel_regularizer)(activation)\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "    import keras.backend as K\n",
    "    from keras.layers import Conv1D\n",
    "    from keras.layers.merge import add\n",
    "    from keras.regularizers import l2\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    \n",
    "    strides = int(round(input_shape[1] / residual_shape[1]))\n",
    "    equal_channels = input_shape[2] == residual_shape[2]\n",
    "    \n",
    "    shortcut = input\n",
    "    # 1 * 1 conv if shape is different. Else identity\n",
    "    if strides > 1 or not equal_channels:\n",
    "        shortcut = Conv1D(filters = residual_shape[2],\n",
    "                         kernel_size = 1,\n",
    "                         strides = strides,\n",
    "                         padding = \"valid\",\n",
    "                         kernel_initializer = \"he_normal\",\n",
    "                         kernel_regularizer = l2(1e-4))(input)\n",
    "    return add([shortcut, residual])\n",
    "    \n",
    "    \n",
    "def basic_block(filters, init_strides = 1, is_first_block_of_first_layer = False):\n",
    "    from keras.regularizers import l2\n",
    "    from keras.layers import Conv1D\n",
    "    def f(input):\n",
    "        if is_first_block_of_first_layer:\n",
    "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
    "            conv1 = Conv1D(filters = filters, kernel_size = 3,\n",
    "                          strides = init_strides,\n",
    "                          padding = \"same\",\n",
    "                          kernel_initializer = \"he_normal\",\n",
    "                          kernel_regularizer = l2(1e-4))(input)\n",
    "        else:\n",
    "            conv1 = _bn_relu_conv(filters = filters, kernel_size = 3,\n",
    "                                 strides = init_strides)(input)\n",
    "\n",
    "        residual = _bn_relu_conv(filters = filters, kernel_size = 3)(conv1)\n",
    "        return _shortcut(input, residual)\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "def _residual_block(filters, repetitions, is_first_layer = False):\n",
    "    def f(input):\n",
    "        for i in range(repetitions):\n",
    "            init_strides = 1\n",
    "            if i == 0 and not is_first_layer:\n",
    "                init_strides = 2\n",
    "            input = basic_block(filters, init_strides = init_strides, is_first_block_of_first_layer = (is_first_layer and i == 0))(input)\n",
    "        return input\n",
    "    \n",
    "    return f\n",
    "        \n",
    "    \n",
    "def _conv_bn_relu(**conv_params):\n",
    "    from keras.layers import Conv1D\n",
    "    from keras.regularizers import l2\n",
    "    filters = conv_params['filters']\n",
    "    kernel_size = conv_params['kernel_size']\n",
    "    strides = conv_params.setdefault('strides', 1)\n",
    "    kernel_initializer = conv_params.setdefault('kernel_initializer', 'he_normal')\n",
    "    padding = conv_params.setdefault('padding', 'same')\n",
    "    kernel_regularizer = conv_params.setdefault('kernel_regularizer', l2(1e-4))\n",
    "    \n",
    "    def f(input):\n",
    "        conv = Conv1D(filters = filters, kernel_size = kernel_size,\n",
    "                     strides = strides, padding = padding,\n",
    "                     kernel_initializer = kernel_initializer,\n",
    "                     kernel_regularizer = kernel_regularizer)(input)\n",
    "        return _bn_relu(conv)\n",
    "    return f\n",
    "\n",
    "\n",
    "def build_model(embedding_weights):\n",
    "    from keras.layers import Input, MaxPooling1D, AveragePooling1D, Flatten, Dense, Embedding\n",
    "    from keras.models import Model\n",
    "    from keras.optimizers import Adam\n",
    "    import keras.backend as K\n",
    "    input = Input(shape = (500, ))\n",
    "    embedding = Embedding(embedding_weights.shape[0], embedding_weights.shape[1], input_length = 500, name = 'embedding')(input)\n",
    "    conv1 = _conv_bn_relu(filters = 64, kernel_size = 5, strides = 1)(embedding)\n",
    "    pool1 = MaxPooling1D(pool_size = 3, strides = 2, padding = \"same\")(conv1)\n",
    "    \n",
    "    block = pool1\n",
    "    filters = 64\n",
    "    for i, r in enumerate([2, 2, 2, 2]):\n",
    "        block = _residual_block(filters, repetitions = r, is_first_layer = (i == 0))(block)\n",
    "        filters *= 2\n",
    "        \n",
    "    block = _bn_relu(block)\n",
    "    \n",
    "    # Classifier block\n",
    "    block_shape = K.int_shape(block)\n",
    "    pool2 = AveragePooling1D(pool_size = (block_shape[1]), strides = 1)(block)\n",
    "    flatten1 = Flatten()(pool2)\n",
    "    dense = Dense(1, kernel_initializer = \"he_normal\", activation = \"sigmoid\")(flatten1)\n",
    "    model = Model(inputs = input, outputs = dense)\n",
    "    \n",
    "    optimizer = Adam(lr = 0.001, clipnorm = 1)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = [calc_precision_score, calc_recall_score, calc_f1_score])\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    embedding_layer.set_weights([embedding_weights])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "stopping = keras.callbacks.EarlyStopping(patience = 8)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(factor = 0.1, patience = 2, min_lr = 0.001 * 0.001)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = \"./text-cnn.model\",\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs = 20, batch_size = 32, validation_data = (X_dev, y_dev), \n",
    "                 callbacks = [stopping, reduce_lr, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1914\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             )\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     )\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1343\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot': 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     prog=prog)\n\u001b[0;32m-> 1922\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1923\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-30f0a84d090e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_auth.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \"\"\"\n\u001b[1;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[0;32m--> 240\u001b[0;31m                        expand_nested, dpi)\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         raise OSError(\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file = 'model_auth.png', show_shapes= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
